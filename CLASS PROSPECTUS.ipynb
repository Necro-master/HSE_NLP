{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADDITIONAL FUNCTIONS\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "pStopWordsList = stopwords.words('english') + stopwords.words('russian')\n",
    "    \n",
    "def process_text_layer(TextLayer, pStopWordsList = []):\n",
    "    txt =  \" \".join([w for w in TextLayer.lower().split() \\\n",
    "                     if (not w in pStopWordsList)])\n",
    "\n",
    "    txt = txt. \\\n",
    "        replace('«', ''). \\\n",
    "        replace('»', ''). \\\n",
    "        replace('(', ''). \\\n",
    "        replace(')', ''). \\\n",
    "        replace('\\[)', ''). \\\n",
    "        replace('\\]', ''). \\\n",
    "        replace('^', ''). \\\n",
    "        replace('\\\\', '')\n",
    "\n",
    "    return txt\n",
    "\n",
    "def learnBinaryClassifier(ds):\n",
    "    '''Kлассификатор. \n",
    "   Input: ds (dataframe): x - текстовый слой; target - результат\n",
    "   Output: CountVectorizer, RandomForestClassifier\n",
    "   '''\n",
    "    ngram_range = (1, 3)\n",
    "    max_features = 50000\n",
    "    n_estimators=100\n",
    "\n",
    "    print(\"Create vectorizer\")\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                tokenizer = None,\n",
    "                                preprocessor = None,\n",
    "                                stop_words = None, \n",
    "                                ngram_range = ngram_range,\n",
    "                                max_features = max_features\n",
    "                                )\n",
    "\n",
    "    print(\"Fit and Transform vectorizer\")\n",
    "\n",
    "    X_train = ds['x']\n",
    "    Y_train = ds['target']\n",
    "\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_train = X_train.toarray()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "    print(\"Fit Model\")\n",
    "    model = model.fit(X_train, Y_train)\n",
    "\n",
    "    return vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv('CLASS PROSPECTUS.csv')\n",
    "text_data['string_value'] = text_data['string_value'].astype(str)\n",
    "class_data = pd.read_excel('CLASS PROSPECTUS.xlsx', sheet_name = 'DATA')\n",
    "\n",
    "models_rename = {\n",
    "    \"Ограничение по предоставлению залога\":\"Restriction_on_the_provision_of_collateral\",\n",
    "    \"Изменение контроля\":\"Change_of_control\",\n",
    "    \"Случаи дефолта\":\"Cases_of_default\",\n",
    "    \"Кросс-дефолт\":\"Cross-default\",\n",
    "    \"Оговорки о коллективных действиях\":\"Collective_action_clauses\",\n",
    "    \"Ограничение задолженности\":\"Limitation_on_indebtedness\",\n",
    "    \"Ограничение задолженности дочерних компаний\":\"Limitation_of_debt_of_subsidiaries\",\n",
    "    \"Ограничение по платежам\":\"Limitation_on_payments\",\n",
    "    \"Ограничение по инвестициям\":\"Investment_restriction\",\n",
    "    \"Ограничение по платежам в отношении дочерних компаний\":\"Limitation_on_payments_to_subsidiaries\",\n",
    "    \"Ограничение по транзакциям с аффилированными лицами\":\"Restriction_on_transactions_with_affiliates\",\n",
    "    \"Ограничение деятельности\":\"Restriction_of_activity\",\n",
    "    \"Ограничение по продаже активов\":\"Restriction_on_asset_sales\",\n",
    "    \"Ограничение по продаже активов с обратной арендой\":\"Restriction_on_the_sale_of_assets_with_leaseback\",\n",
    "    \"Ограничение по слиянию\":\"Limitation_on_merger\",\n",
    "    \"Триггер рейтингов\":\"Ratings_trigger\",\n",
    "    \"Обозначение прав дочерних компаний (restricted / unrestricted)\":\"Designation_of_the_rights_of_subsidiaries\",\n",
    "    \"Ограничение по наслоению долговых обязательств по рангам\":\"Restriction_on_the_layering_of_debt_obligations_by_rank\",\n",
    "    \"Условие приостановки действия ковенантов\":\"A_condition_of_suspension_of_the_covenants\",\n",
    "    \"Финансовые ковенанты\":\"Financial_covenants\"}\n",
    "class_data = class_data.rename(columns=models_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уменьшаем размерность задачи, так как слишком много нулевых данных\n",
    "tags = list(class_data['tag'])\n",
    "new_tags = list(text_data[~text_data['tag'].isin(tags)].sample(3000)['tag'])\n",
    "new_tags = new_tags + tags\n",
    "text_data = text_data[text_data['tag'].isin(new_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text_data.merge(class_data, on='tag', how='left').copy()\n",
    "data['string_value'] = data.apply(lambda r: process_text_layer(r['string_value'], pStopWordsList), axis = 1)\n",
    "models = list(data.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "#models = ['Change_of_control']\n",
    "for model_name in models:\n",
    "    ds = data[['tag','string_value', model_name]].rename(columns={'string_value': 'x', model_name: 'target'})\n",
    "    ds = ds.fillna(0)\n",
    "    ds = ds[ds['target']!=''].drop_duplicates()\n",
    "    datasets[model_name] = ds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_part = 0.2\n",
    "for model_name in models:\n",
    "    df_validation = datasets[model_name].sample(round(len(datasets[model_name])*validation_set_part)).copy()\n",
    "    item = {\n",
    "        'df_validation': df_validation.copy(),\n",
    "        'df_training': datasets[model_name][datasets[model_name]['tag'].isin(df_validation['tag'].values)==False].copy()\n",
    "    }\n",
    "    datasets[model_name] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n",
      "Create vectorizer\n",
      "Fit and Transform vectorizer\n",
      "Fit Model\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    vector, model = learnBinaryClassifier(datasets[model_name]['df_training'])\n",
    "    with open(('models\\\\CLASS_PROSPECTUS_%s.rft' % model_name), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(('models\\\\VECTOR_PROSPECTUS_%s.txt' % model_name), 'wb') as f:\n",
    "        pickle.dump(vector, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Restriction_on_the_provision_of_collateral statistic (validation rows 685): Precision= 98.83 ; To_Validate=0.0  at Confidence=0\n",
      "Model Change_of_control statistic (validation rows 685): Precision= 99.12 ; To_Validate=0.0  at Confidence=0\n",
      "Model Cases_of_default statistic (validation rows 685): Precision= 98.98 ; To_Validate=0.0  at Confidence=0\n",
      "Model Cross-default statistic (validation rows 685): Precision= 98.54 ; To_Validate=0.0  at Confidence=0\n",
      "Model Collective_action_clauses statistic (validation rows 685): Precision= 98.54 ; To_Validate=0.0  at Confidence=0\n",
      "Model Limitation_on_indebtedness statistic (validation rows 685): Precision= 99.71 ; To_Validate=0.0  at Confidence=0\n",
      "Model Limitation_of_debt_of_subsidiaries statistic (validation rows 685): Precision= 99.85 ; To_Validate=0.0  at Confidence=0\n",
      "Model Limitation_on_payments statistic (validation rows 685): Precision= 99.71 ; To_Validate=0.0  at Confidence=0\n",
      "Model Investment_restriction statistic (validation rows 685): Precision= 100.0 ; To_Validate=0.0  at Confidence=0\n",
      "Model Limitation_on_payments_to_subsidiaries statistic (validation rows 685): Precision= 100.0 ; To_Validate=0.0  at Confidence=0\n",
      "Model Restriction_on_transactions_with_affiliates statistic (validation rows 685): Precision= 99.85 ; To_Validate=0.0  at Confidence=0\n",
      "Model Restriction_of_activity statistic (validation rows 685): Precision= 99.71 ; To_Validate=0.0  at Confidence=0\n",
      "Model Restriction_on_asset_sales statistic (validation rows 685): Precision= 99.85 ; To_Validate=0.0  at Confidence=0\n",
      "Model Restriction_on_the_sale_of_assets_with_leaseback statistic (validation rows 685): Precision= 99.71 ; To_Validate=0.0  at Confidence=0\n",
      "Model Limitation_on_merger statistic (validation rows 685): Precision= 99.27 ; To_Validate=0.0  at Confidence=0\n",
      "Model Ratings_trigger statistic (validation rows 685): Precision= 100.0 ; To_Validate=0.0  at Confidence=0\n",
      "Model Обозначение прав дочерних компаний (restricted unrestricted) statistic (validation rows 685): Precision= 100.0 ; To_Validate=0.0  at Confidence=0\n",
      "Model Restriction_on_the_layering_of_debt_obligations_by_rank statistic (validation rows 685): Precision= 99.85 ; To_Validate=0.0  at Confidence=0\n",
      "Model A_condition_of_suspension_of_the_covenants statistic (validation rows 685): Precision= 99.27 ; To_Validate=0.0  at Confidence=0\n",
      "Model Financial_covenants statistic (validation rows 685): Precision= 99.42 ; To_Validate=0.0  at Confidence=0\n"
     ]
    }
   ],
   "source": [
    "confidence_level = 0\n",
    "for model_name in models:\n",
    "    with open(('models\\\\CLASS_PROSPECTUS_%s.rft' % model_name), 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(('models\\\\VECTOR_PROSPECTUS_%s.txt' % model_name), 'rb') as f:\n",
    "        vector = pickle.load(f)  \n",
    "        \n",
    "    ds = datasets[model_name]['df_validation']\n",
    "    \n",
    "    ds['predict']= model.predict(vector.transform(ds['x']))\n",
    "    ds['confidence']=np.transpose(np.amax(model.predict_proba(vector.transform(ds['x'])), axis=1))\n",
    "    ds['target'] = ds.apply(lambda r: 'empty' if r['target']==None else r['target'], axis=1)\n",
    "    \n",
    "    precision = round(100- 100*len(ds[(ds['confidence']>=confidence_level)&(ds['target']!=ds['predict'])])/len(ds),2)\n",
    "    to_validate = round(100*len(ds[(ds['confidence']<confidence_level)])/len(ds),2)\n",
    "    \n",
    "    print('Model %s statistic (validation rows %s): Precision= %s ; To_Validate=%s  at Confidence=%s' % (model_name, len(ds), precision, to_validate, confidence_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
