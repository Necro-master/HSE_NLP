{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_messages(messages):\n",
    "    for i, m in enumerate(messages, 1):\n",
    "        trange(1, desc=str(m), position=i, bar_format='{desc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADDITIONAL FUNCTIONS\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "# IVL: добавил дирректорию для выгрузки ошибок    \n",
    "if not os.path.exists('errors'):\n",
    "    os.makedirs('errors')   \n",
    "# IVL: добавил дирректорию для выгрузки примеров для проверки\n",
    "if not os.path.exists('to_validate'):\n",
    "    os.makedirs('to_validate')       \n",
    "\n",
    "#pStopWordsList = stopwords.words('english') + stopwords.words('russian')\n",
    "#IVL: Чтобы локализовать запуск версии и зафиксировать стоп слова на русском и английском, загружаем списки из файлов\n",
    "pStopWordsList = open(\"StopwordList_en.txt\", \"r\").read().splitlines() + \\\n",
    "open(\"StopwordList_ru.txt\", \"r\", encoding = 'cp1251').read().splitlines()\n",
    "    \n",
    "def process_text_layer(TextLayer, pStopWordsList = []):\n",
    "    txt =  \" \".join([w for w in TextLayer.lower().split() \\\n",
    "                     if (not w in pStopWordsList)])\n",
    "\n",
    "    txt = txt. \\\n",
    "        replace('«', ''). \\\n",
    "        replace('»', ''). \\\n",
    "        replace('(', ''). \\\n",
    "        replace(')', ''). \\\n",
    "        replace('\\[)', ''). \\\n",
    "        replace('\\]', ''). \\\n",
    "        replace('^', ''). \\\n",
    "        replace('\\\\', '')\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnBinaryClassifier_TF_IDF(ds):\n",
    "    '''Kлассификатор. \n",
    "   Input: ds (dataframe): x - текстовый слой; target - результат\n",
    "   Output: CountVectorizer, RandomForestClassifier\n",
    "   '''\n",
    "    ngram_range = (1, 3)\n",
    "    max_features = 50000\n",
    "    n_estimators=100\n",
    "\n",
    "    #print(\"Create vectorizer\")\n",
    "    vectorizer = TfidfVectorizer(analyzer = \"word\",\n",
    "                                tokenizer = None,\n",
    "                                preprocessor = None,\n",
    "                                stop_words = None, \n",
    "                                ngram_range = ngram_range,\n",
    "                                max_features = max_features\n",
    "                                )\n",
    "\n",
    "    #print(\"Fit and Transform vectorizer\")\n",
    "\n",
    "    X_train = ds['x']\n",
    "    Y_train = ds['target']\n",
    "\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_train = X_train.toarray()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "    #print(\"Fit Model\")\n",
    "    model = model.fit(X_train, Y_train)\n",
    "\n",
    "    return vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnBinaryClassifier_CountVectorizer(ds):\n",
    "    '''Kлассификатор. \n",
    "   Input: ds (dataframe): x - текстовый слой; target - результат\n",
    "   Output: CountVectorizer, RandomForestClassifier\n",
    "   '''\n",
    "    ngram_range = (1, 3)\n",
    "    max_features = 50000\n",
    "    n_estimators=100\n",
    "\n",
    "    #print(\"Create vectorizer\")\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                tokenizer = None,\n",
    "                                preprocessor = None,\n",
    "                                stop_words = None, \n",
    "                                ngram_range = ngram_range,\n",
    "                                max_features = max_features\n",
    "                                )\n",
    "\n",
    "    #print(\"Fit and Transform vectorizer\")\n",
    "\n",
    "    X_train = ds['x']\n",
    "    Y_train = ds['target']\n",
    "\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_train = X_train.toarray()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators)\n",
    "\n",
    "    #print(\"Fit Model\")\n",
    "    model = model.fit(X_train, Y_train)\n",
    "\n",
    "    return vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv('CLASS PROSPECTUS.csv')\n",
    "text_data['string_value'] = text_data['string_value'].astype(str)\n",
    "class_data = pd.read_excel('CLASS PROSPECTUS.xlsx', sheet_name = 'DATA')\n",
    "\n",
    "models_rename = {\n",
    "    \"Ограничение по предоставлению залога\":\"Restriction_on_the_provision_of_collateral\",\n",
    "    \"Изменение контроля\":\"Change_of_control\",\n",
    "    \"Случаи дефолта\":\"Cases_of_default\",\n",
    "    \"Кросс-дефолт\":\"Cross-default\",\n",
    "    \"Оговорки о коллективных действиях\":\"Collective_action_clauses\",\n",
    "    \"Ограничение задолженности\":\"Limitation_on_indebtedness\",\n",
    "    \"Ограничение задолженности дочерних компаний\":\"Limitation_of_debt_of_subsidiaries\",\n",
    "    \"Ограничение по платежам\":\"Limitation_on_payments\",\n",
    "    \"Ограничение по инвестициям\":\"Investment_restriction\",\n",
    "    \"Ограничение по платежам в отношении дочерних компаний\":\"Limitation_on_payments_to_subsidiaries\",\n",
    "    \"Ограничение по транзакциям с аффилированными лицами\":\"Restriction_on_transactions_with_affiliates\",\n",
    "    \"Ограничение деятельности\":\"Restriction_of_activity\",\n",
    "    \"Ограничение по продаже активов\":\"Restriction_on_asset_sales\",\n",
    "    \"Ограничение по продаже активов с обратной арендой\":\"Restriction_on_the_sale_of_assets_with_leaseback\",\n",
    "    \"Ограничение по слиянию\":\"Limitation_on_merger\",\n",
    "    \"Обозначение прав дочерних компаний (restricted / unrestricted)\":\"Designation_of_the_rights_of_subsidiaries\",\n",
    "    \"Ограничение по наслоению долговых обязательств по рангам\":\"Restriction_on_the_layering_of_debt_obligations_by_rank\",\n",
    "    \"Условие приостановки действия ковенантов\":\"A_condition_of_suspension_of_the_covenants\",\n",
    "    \"Финансовые ковенанты\":\"Financial_covenants\"}\n",
    "class_data = class_data.rename(columns=models_rename)\n",
    "print(class_data['tag'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уменьшаем размерность задачи, так как слишком много нулевых данных\n",
    "tags = list(class_data['tag'])\n",
    "new_tags = list(text_data[~text_data['tag'].isin(tags)].sample(3000)['tag'])\n",
    "new_tags = new_tags + tags\n",
    "text_data = text_data[text_data['tag'].isin(new_tags)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = text_data.merge(class_data, on='tag', how='left').copy()\n",
    "data['string_value'] = data.apply(lambda r: process_text_layer(r['string_value'], pStopWordsList), axis = 1)\n",
    "models = list(data.columns)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "#models = ['Change_of_control']\n",
    "for model_name in models:\n",
    "    ds = data[['tag','string_value', model_name]].rename(columns={'string_value': 'x', model_name: 'target'})\n",
    "    ds = ds.fillna(0)\n",
    "    ds = ds[ds['target']!=''].drop_duplicates()\n",
    "    datasets[model_name] = ds.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set_part = 0.1\n",
    "for model_name in models:\n",
    "    df_validation = datasets[model_name].sample(round(len(datasets[model_name])*validation_set_part)).copy()\n",
    "    item = {\n",
    "        'df_validation': df_validation.copy(),\n",
    "        'df_training': datasets[model_name][datasets[model_name]['tag'].isin(df_validation['tag'].values)==False].copy()\n",
    "    }\n",
    "    datasets[model_name] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#IVL: Немного детализировал отображение статистики\n",
    "#tbar = tqdm(range(len(models)))\n",
    "for i in trange(len(models)):\n",
    "    model_name = models[i]\n",
    "    line_messages(['iter: %s' % i, 'model: %s' % (model_name)])\n",
    "    #tqdm.write(f'Count vectorized {model_name} created')\n",
    "    vector, model = learnBinaryClassifier_CountVectorizer(datasets[model_name]['df_training'])\n",
    "    with open(('models/CLASS_PROSPECTUS_%s_CountVectorizer.rft' % model_name), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(('models/VECTOR_PROSPECTUS_%s_CountVectorizer.txt' % model_name), 'wb') as f:\n",
    "        pickle.dump(vector, f)\n",
    "    #tqdm.write(f'Count vectorizer {model_name} dumped')\n",
    "    #print('Save CountVectorized model (%s/%s): %s' % (i+1, len(models), model_name))\n",
    "    #tqdm.write(f'TF-IDF {model_name} created')                 \n",
    "    vector, model = learnBinaryClassifier_TF_IDF(datasets[model_name]['df_training'])\n",
    "    with open(('models/CLASS_PROSPECTUS_%s_TF-IDF.rft' % model_name), 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    with open(('models/VECTOR_PROSPECTUS_%s_TF-IDF.txt' % model_name), 'wb') as f:\n",
    "        pickle.dump(vector, f)\n",
    "    #tqdm.write(f'TF-IDF {model_name} dumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ds_mistakes_countvectorizer = {}\n",
    "ds_mistakes_tfidf = {}\n",
    "\n",
    "ds_to_validate_countvectorizer = {}\n",
    "ds_to_validate_tfidf = {}\n",
    "confidence_level = 0.75\n",
    "\n",
    "for model_name in models:\n",
    "    with open(('models/CLASS_PROSPECTUS_%s_CountVectorizer.rft' % model_name), 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(('models/VECTOR_PROSPECTUS_%s_CountVectorizer.txt' % model_name), 'rb') as f:\n",
    "        vector = pickle.load(f)  \n",
    "    \n",
    "    ds = datasets[model_name]['df_validation']\n",
    "    \n",
    "    ds['predict']= model.predict(vector.transform(ds['x']))\n",
    "    ds['confidence']=np.transpose(np.amax(model.predict_proba(vector.transform(ds['x'])), axis=1))\n",
    "    ds['target'] = ds.apply(lambda r: 'empty' if r['target']==None else r['target'], axis=1)\n",
    "    ds_mistakes_countvectorizer[model_name] = ds[(ds['confidence']>=confidence_level)&(ds['target']!=ds['predict'])]\n",
    "    #IVL: добавлено сохранение примера для валидации\n",
    "    ds_to_validate_countvectorizer[model_name] = ds[(ds['confidence']<confidence_level)]\n",
    "    \n",
    "    precision_countvectorizer = round(100- 100*len(ds[(ds['confidence']>=confidence_level)&(ds['target']!=ds['predict'])])/len(ds),2)\n",
    "    to_validate = round(100*len(ds[(ds['confidence']<confidence_level)])/len(ds),2)\n",
    "    \n",
    "    print('Model CountVectorizer %s statistic (validation rows %s): Precision= %s ; To_Validate=%s  at Confidence=%s' % (model_name, len(ds), precision_countvectorizer, to_validate, confidence_level))\n",
    "    \n",
    "    #TF-IDF\n",
    "    \n",
    "    with open(('models/CLASS_PROSPECTUS_%s_TF-IDF.rft' % model_name), 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(('models/VECTOR_PROSPECTUS_%s_TF-IDF.txt' % model_name), 'rb') as f:\n",
    "        vector = pickle.load(f)  \n",
    "    \n",
    "    ds = datasets[model_name]['df_validation']\n",
    "    \n",
    "    ds['predict']= model.predict(vector.transform(ds['x']))\n",
    "    ds['confidence']=np.transpose(np.amax(model.predict_proba(vector.transform(ds['x'])), axis=1))\n",
    "    ds['target'] = ds.apply(lambda r: 'empty' if r['target']==None else r['target'], axis=1)\n",
    "    ds_mistakes_tfidf[model_name] = ds[(ds['confidence']>=confidence_level)&(ds['target']!=ds['predict'])]\n",
    "    #IVL: добавлено сохранение примера для валидации\n",
    "    ds_to_validate_tfidf[model_name] = ds[(ds['confidence']<confidence_level)]\n",
    "    \n",
    "    precision_tfidf = round(100- 100*len(ds[(ds['confidence']>=confidence_level)&(ds['target']!=ds['predict'])])/len(ds),2)\n",
    "    to_validate = round(100*len(ds[(ds['confidence']<confidence_level)])/len(ds),2)\n",
    "    \n",
    "    print('Model TF-IDF %s statistic (validation rows %s): Precision= %s ; To_Validate=%s  at Confidence=%s' % (model_name, len(ds), precision_tfidf, to_validate, confidence_level))\n",
    "    \n",
    "    print(f'Best Precision= {max(precision_countvectorizer, precision_tfidf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IVL: удаляем файлы с прошлого запуска\n",
    "if os.path.exists('errors'):\n",
    "    for root, dirs, files in os.walk('errors'):\n",
    "        for file in files:\n",
    "            os.remove(f'{root}//{file}')\n",
    "#IVL: выгружаю только ошибки, пустые таблицы не выгружать\n",
    "for name in models:\n",
    "    if len(ds_mistakes_countvectorizer[name])>0: ds_mistakes_countvectorizer[name].to_excel(f'errors//{name}_CountVectorizer.xlsx')\n",
    "    if len(ds_mistakes_tfidf[name])>0: ds_mistakes_tfidf[name].to_excel(f'errors//{name}_TF-IDF.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IVL: выгружаем примеры для валидации\n",
    "#IVL: удаляем файлы с прошлого запуска\n",
    "if os.path.exists('to_validate'):\n",
    "    for root, dirs, files in os.walk('to_validate'):\n",
    "        for file in files:\n",
    "            os.remove(f'{root}//{file}')\n",
    "#IVL: выгружаю только примеры для валидации, пустые таблицы не выгружать\n",
    "for name in models:\n",
    "    if len(ds_to_validate_countvectorizer[name])>0: ds_to_validate_countvectorizer[name].to_excel(f'to_validate//{name}_CountVectorizer.xlsx')\n",
    "    if len(ds_to_validate_tfidf[name])>0: ds_to_validate_tfidf[name].to_excel(f'to_validate//{name}_TF-IDF.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
